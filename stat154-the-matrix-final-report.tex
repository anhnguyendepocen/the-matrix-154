\documentclass[]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
    \usepackage{xltxtra,xunicode}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Mapping=tex-text,Scale=MatchLowercase}
  \newcommand{\euro}{€}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage[margin=1in]{geometry}
\usepackage{longtable,booktabs}
\ifxetex
  \usepackage[setpagesize=false, % page size defined by xetex
              unicode=false, % unicode breaks when used with xetex
              xetex]{hyperref}
\else
  \usepackage[unicode=true]{hyperref}
\fi
\hypersetup{breaklinks=true,
            bookmarks=true,
            pdfauthor={},
            pdftitle={STAT 154 Final Report},
            colorlinks=true,
            citecolor=blue,
            urlcolor=blue,
            linkcolor=magenta,
            pdfborder={0 0 0}}
\urlstyle{same}  % don't use monospace font for urls
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\setcounter{secnumdepth}{0}

%%% Use protect on footnotes to avoid problems with footnotes in titles
\let\rmarkdownfootnote\footnote%
\def\footnote{\protect\rmarkdownfootnote}

%%% Change title format to be more compact
\usepackage{titling}

% Create subtitle command for use in maketitle
\newcommand{\subtitle}[1]{
  \posttitle{
    \begin{center}\large#1\end{center}
    }
}

\setlength{\droptitle}{-2em}
  \title{STAT 154 Final Report}
  \pretitle{\vspace{\droptitle}\centering\huge}
  \posttitle{\par}
  \author{The Matrix (Team 5)\\Stephanie Kim, Chris Kennedy, Dongkyun Kim, Saejin
Kim, Zhenzheng Hu}
  \preauthor{\centering\large\emph}
  \postauthor{\par}
  \predate{\centering\large\emph}
  \postdate{\par}
  \date{12/6/2015}



\begin{document}

\maketitle


\subsection{{1. Description}}\label{description}

This project consists of an automated text categorization task
(Sebastiani 2002). The data is classified into 4 genres; ``Child'',
``History'', ``Religion'', and ``Science''. Each category contains
fractions of text files from that genre. ``Child'' category has 7,165
files, ``History'' category has 5,353 files, ``Religion'' category has
2,362 files, and ``Science'' category has 7,432 files. It is plausible
for human to classify the text files from their contexts or specific
words. The goal of this project is to train the machine so that it can
classify the texts as how humans do. We use support vector machine and
random forest methods, test on new text files, and measure the test
error of each method.

\begin{longtable}[c]{@{}ll@{}}
\toprule
Categories & Number.of.files\tabularnewline
\midrule
\endhead
Child & 7,165 files\tabularnewline
History & 5,353 files\tabularnewline
Religion & 2,362 files\tabularnewline
Science & 7,432 files\tabularnewline
\bottomrule
\end{longtable}

\subsection{{2. Feature Creation}}\label{feature-creation}

In order to do text mining, we loaded the text data as a corpus, which
is a collection of separate texts. The tm package provides several text
transformation or word count commands. The tm\_map() command is applied
to the corpus to make transformations such as excluding too common or
rare words, convert every word to lowercase, and remove punctuations or
words. We stemmed the words to consolidate slight variations of word
suffixes into a more concise feature set (Porter 1980). We also removed
the lengthy header and footer text created by Project Gutenberg that
would likely hurt our predictive accuracy.

The DocumentTermMatrix() command is then applied to make a dictionary of
every unique word of the loaded text files and count the appearances of
each word. The outcome is in form of a data frame. Thus each row is the
text file, each column is the unique word, and each cell is the word
count in specific text file (22308 * 12262 data frame). This word
feature matrix approach is commonly known as a bag of words model
(Manning \& Schütze 1999, 7.2).

We then generated a target vector as a factor with values of ``Child'',
``History'', ``Religion'', and ``Science'' which are the tags of the
texts' genre.

\subsection{{3. Unsupervised Feature
Filtering}}\label{unsupervised-feature-filtering}

Before applying the minimum or maximum threshold to the feature matrix,
we check if specific word is included in each text. In other words we
count how many documents use that word. The maximum threshold we applied
is if a word is in at least 70\% of documents. Here we removed 11 words.
The minimum threshold is if a word is in at least 200 documents. The
number of word features we ended with is 7159 features. So our feature
matrix is 22308 * 7159 data frame.

\subsection{{4. Power Feature
Extraction}}\label{power-feature-extraction}

The power features are the variables that help distinguish files in
different categories. These features consist of three kinds: Word-based
power features, sentence-based power features, and N-grams.The following
are the features from each file considered to help prediction.

Before applying the minimum or maximum threshold to the feature matrix,
we check if specific word is included in each text. In other words we
count how many documents use that word. The maximum threshold we applied
is if a word is in at least 70\% of documents. Here we removed 11 words.
The minimum threshold is if a word is in at least 200 documents. The
number of word features we ended with is 7159 features. So our feature
matrix is 22308 * 7159 data frame.

\subsection{{5. Classification of Power Word Feature
Matrix}}\label{classification-of-power-word-feature-matrix}

\begin{enumerate}
\def\labelenumi{(\arabic{enumi})}
\item
  Word-based power features: Features are collected using the unfiltered
  word matrix, which presents the number of appearance of every word in
  a text.
\item
  Sentence-based power features: We first splitted the text files into
  sentences and then collected the power features by inspecting the list
  of sentences.
\item
  N-grams: N-gram is a sequence of N words sliced from a longer string.
  The word features we described above are unigrams. It is known that
  the N-gram-based approach to text categorization is tolerant of
  textual errors, fast and robust. This approach works very well for
  language classification (William \& John, 1994). In this project, for
  better prediction, other than unigrams we also included the bigram
  trigram features. We were able to make a feature matrix of N-grams
  with the DocumentTermMatrix() command as we did for the word features
  but by adjusting the command with the N-gram-tokenizer.
\end{enumerate}

\subsection{{5. Word and Power Feature
Combination}}\label{word-and-power-feature-combination}

\subsection{{6. Classification of Word Feature
Matrix}}\label{classification-of-word-feature-matrix}

\subsection{{7. Classification of Power Word Feature
Matrix}}\label{classification-of-power-word-feature-matrix-1}

\subsection{{8. Classification of Combined Feature
Matrix}}\label{classification-of-combined-feature-matrix}

\subsection{{9. Final Model}}\label{final-model}

\subsection{{10. Validation Set}}\label{validation-set}

\subsection{{11. References}}\label{references}

\end{document}
