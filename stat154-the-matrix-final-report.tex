\documentclass[12pt,]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
    \usepackage{xltxtra,xunicode}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Mapping=tex-text,Scale=MatchLowercase}
  \newcommand{\euro}{€}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage[margin=1in]{geometry}
\usepackage{longtable,booktabs}
\ifxetex
  \usepackage[setpagesize=false, % page size defined by xetex
              unicode=false, % unicode breaks when used with xetex
              xetex]{hyperref}
\else
  \usepackage[unicode=true]{hyperref}
\fi
\hypersetup{breaklinks=true,
            bookmarks=true,
            pdfauthor={},
            pdftitle={STAT 154 Final Report},
            colorlinks=true,
            citecolor=blue,
            urlcolor=blue,
            linkcolor=magenta,
            pdfborder={0 0 0}}
\urlstyle{same}  % don't use monospace font for urls
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\setcounter{secnumdepth}{0}

%%% Use protect on footnotes to avoid problems with footnotes in titles
\let\rmarkdownfootnote\footnote%
\def\footnote{\protect\rmarkdownfootnote}

%%% Change title format to be more compact
\usepackage{titling}

% Create subtitle command for use in maketitle
\newcommand{\subtitle}[1]{
  \posttitle{
    \begin{center}\large#1\end{center}
    }
}

\setlength{\droptitle}{-2em}
  \title{STAT 154 Final Report}
  \pretitle{\vspace{\droptitle}\centering\huge}
  \posttitle{\par}
  \author{The Matrix (Team5)\\Stephanie Kim (25937115), Chris Kennedy
(25947847),\\Dongkyun Kim (22874163), Saejin Kim (00000000), Zhenzheng
Hu (23715262)}
  \preauthor{\centering\large\emph}
  \postauthor{\par}
  \predate{\centering\large\emph}
  \postdate{\par}
  \date{12/7/2015}



\begin{document}

\maketitle


{
\hypersetup{linkcolor=black}
\setcounter{tocdepth}{2}
\tableofcontents
}
\subsection{{1. Description}}\label{description}

This project consists of an automated text categorization task
(Sebastiani 2002) applied to text documents collected from Project
Gutenberg, a free ebook archive. The documents are categorized into 4
genres: ``Child'', ``History'', ``Religion'', and ``Science''. A human
would be able to classify these documents without much effort but it
would be very time confusing to do so for thousands or millions of
documents. The goal of this project is to develop a machine learning
classifier to correctly categorize the texts as accurately as possible.
Our primary feature set is a word-feature matrix which counts the number
of a times each word occurs in a document, known as a bag of words
model. We extend this word feature matrix with bigrams, trigrams, and
meta-features related to statistics on word and character usage. We use
support vector machine and random forest methods, test on new text
files, and measure the test error of each method. Our final model is a
gradient boosted machine.

\begin{longtable}[c]{@{}ll@{}}
\caption{Documents in each category}\tabularnewline
\toprule
Categories & Number of files\tabularnewline
\midrule
\endfirsthead
\toprule
Categories & Number of files\tabularnewline
\midrule
\endhead
Child & 7,165\tabularnewline
History & 5,353\tabularnewline
Religion & 2,362\tabularnewline
Science & 7,432\tabularnewline
\bottomrule
\end{longtable}

\subsection{{2. Feature Creation}}\label{feature-creation}

In order to do text mining, we loaded the text data as a corpus, which
is a collection of separate texts. The tm package provides several text
transformation or word count commands. The tm\_map() command is applied
to the corpus to make transformations such as excluding too common or
rare words, convert every word to lowercase, and remove punctuations or
words. We stemmed the words to consolidate slight variations of word
suffixes into a more concise feature set (Porter 1980). Terms that
occurred in fewer than 1\% of documents were excluded at this stage.

We also removed the lengthy header and footer text created by Project
Gutenberg that would likely hurt our predictive accuracy. This was
particularly time-consuming because there is no standard string that
separates the meta-data from the literary data in Project Gutenberg
documents; we strongly recommend that Project Gutenberg develop such a
standard to facilitate comptuer processing of its documents in the
future. Moreover, a set of documents, roughly 600 or more, consisted
entirely of Project Gutenberg legal or other non-literary text. We
removed some of those documents as training examples although we would
have liked to spend more time removing the remainder (e.g.~using outlier
detection methods) so that their feature distributions would not
influence the prediction algorithms.

The DocumentTermMatrix() command is then applied to make a dictionary of
every unique word of the loaded text files and count the appearances of
each word. The outcome is in the form of a data frame, where each row is
a text document, each column is a unique word, and each cell is the
count of that word in the text document. The resulting data frame has
22,305 rows and 12,196 columns. This word feature matrix approach is
commonly known as a bag of words representation (Manning \& Schütze
1999, 7.2).

We then generated a target vector as a factor with values of ``Child'',
``History'', ``Religion'', and ``Science'' which are the tags of the
texts' genre.

\subsection{{3. Unsupervised Feature
Filtering}}\label{unsupervised-feature-filtering}

In order to reduce the feature set size and limit overfitting, we tally
how many documents use each word. We chose a minimum threshold of 50
documents; any word with less usage was removed from the feature matrix.
We selected a relatively low minimum so that the machine learning
algorithms could achieve maximum accuracy through a large feature set -
aggressive pruning would limit our ability to detect patterns in word
usage. Similarly, we chose not to limit on the maximum number of
documents using a feature under the theory that words with high document
coverage could still provide useful differentiation across categories.
Our final word feature matrix contains 22,305 rows and 11,218
word-feature columns.

{[}Plot of word frequency{]}

\subsection{{4. Power Feature
Extraction}}\label{power-feature-extraction}

The power features are the variables that help distinguish files in
different categories. These features consist of three kinds: Word-based
power features, sentence-based power features, and N-grams.The following
are the features from each file considered to help prediction.

\begin{enumerate}
\def\labelenumi{(\arabic{enumi})}
\itemsep1pt\parskip0pt\parsep0pt
\item
  Word-based power features: 6 features are collected using the
  unfiltered word matrix, which presents the number of appearance of
  every word in a text.
\end{enumerate}

\begin{longtable}[c]{@{}ll@{}}
\caption{Word-based power features}\tabularnewline
\toprule
Power Feature & Definition\tabularnewline
\midrule
\endfirsthead
\toprule
Power Feature & Definition\tabularnewline
\midrule
\endhead
words\_count & Number of words\tabularnewline
chars\_count & Number of characters\tabularnewline
words\_avg\_length & Average words length (Number of characters/Number
of words)\tabularnewline
words\_distinct & Number of words that appears in a text at least
once\tabularnewline
sd\_words & Standard deviation of word length\tabularnewline
word\_diversity & Word diversity (Number of distinct words/Number of
words)\tabularnewline
\bottomrule
\end{longtable}

\begin{enumerate}
\def\labelenumi{(\arabic{enumi})}
\setcounter{enumi}{1}
\itemsep1pt\parskip0pt\parsep0pt
\item
  Sentence-based power features: We first split the text files into
  sentences and then collected 6 power features by inspecting the list
  of sentences.
\end{enumerate}

\begin{longtable}[c]{@{}ll@{}}
\caption{Sentence-based power features}\tabularnewline
\toprule
Power Feature & Definition\tabularnewline
\midrule
\endfirsthead
\toprule
Power Feature & Definition\tabularnewline
\midrule
\endhead
sentence\_count & Number of sentences\tabularnewline
sentence\_avg\_length & Average length of sentences\tabularnewline
4digit\_nums & Number of 4-digit number\tabularnewline
digit\_count & Number of digits\tabularnewline
question\_marks & Number of question marks\tabularnewline
exclamation\_points & Number of exclamation points\tabularnewline
\bottomrule
\end{longtable}

\begin{enumerate}
\def\labelenumi{(\arabic{enumi})}
\setcounter{enumi}{2}
\itemsep1pt\parskip0pt\parsep0pt
\item
  N-grams: The N-gram is a sequence of N words sliced from a longer
  string. The word features we described above are unigrams. It is known
  that the N-gram-based approach to text categorization is tolerant of
  textual errors, fast and robust. This approach works very well for
  language classification (William \& John, 1994). In this project, for
  better prediction, other than unigrams we also included the 4,736
  bigram and 497 trigram features. We were able to make a feature matrix
  of N-grams with the DocumentTermMatrix() command as we did for the
  word features but by adjusting the command with the N-gram-tokenizer.
  The trigram features were especially useful for identifying documents
  that only included Project Gutenberg meta text, because non-literary
  legal phrases tended to occur frequently in those documents.
\end{enumerate}

As a result, we have 5,245 power features in total. The power feature
matrix has dimension 22,305 \(\times\) 5,245.

\subsection{{5. Word and Power Feature
Combination}}\label{word-and-power-feature-combination}

We combine our word and power features to generate a final feature
matrix of 22,305 documents and 16,463 features.

\subsection{{6. Classification of Word Feature
Matrix}}\label{classification-of-word-feature-matrix}

\subsection{{7. Classification of Power Word Feature
Matrix}}\label{classification-of-power-word-feature-matrix}

\subsection{{8. Classification of Combined Feature
Matrix}}\label{classification-of-combined-feature-matrix}

\subsection{{9. Final Model}}\label{final-model}

\subsection{{10. Validation Set}}\label{validation-set}

\subsection{{11. References}}\label{references}

Manning, C. D., \& Schütze, H. (1999). Foundations of statistical
natural language processing. MIT press.

Porter, M. F. (1980). An algorithm for suffix stripping. Program, 14(3),
130-137.

Sebastiani, F. (2002). Machine learning in automated text
categorization. ACM computing surveys (CSUR), 34(1), 1-47.

William, B. Cavnar \& John M. Trenkle. (1994). N-Gram-Based Text
Categorization. Environmental Research Institute of Michigan.

\end{document}
