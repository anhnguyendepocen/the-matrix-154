\documentclass[12pt,]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
    \usepackage{xltxtra,xunicode}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Mapping=tex-text,Scale=MatchLowercase}
  \newcommand{\euro}{€}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage[margin=1in]{geometry}
\usepackage{longtable,booktabs}
\ifxetex
  \usepackage[setpagesize=false, % page size defined by xetex
              unicode=false, % unicode breaks when used with xetex
              xetex]{hyperref}
\else
  \usepackage[unicode=true]{hyperref}
\fi
\hypersetup{breaklinks=true,
            bookmarks=true,
            pdfauthor={},
            pdftitle={STAT 154 Final Report},
            colorlinks=true,
            citecolor=blue,
            urlcolor=blue,
            linkcolor=magenta,
            pdfborder={0 0 0}}
\urlstyle{same}  % don't use monospace font for urls
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\setcounter{secnumdepth}{0}

%%% Use protect on footnotes to avoid problems with footnotes in titles
\let\rmarkdownfootnote\footnote%
\def\footnote{\protect\rmarkdownfootnote}

%%% Change title format to be more compact
\usepackage{titling}

% Create subtitle command for use in maketitle
\newcommand{\subtitle}[1]{
  \posttitle{
    \begin{center}\large#1\end{center}
    }
}

\setlength{\droptitle}{-2em}
  \title{STAT 154 Final Report}
  \pretitle{\vspace{\droptitle}\centering\huge}
  \posttitle{\par}
  \author{The Matrix (Team5)\\Stephanie Kim(25937115), Chris
Kennedy(25947847),\\Dongkyun Kim(22874163), Saejin Kim(00000000),
Zhenzheng Hu(23715262)}
  \preauthor{\centering\large\emph}
  \postauthor{\par}
  \predate{\centering\large\emph}
  \postdate{\par}
  \date{12/7/2015}



\begin{document}

\maketitle


{
\hypersetup{linkcolor=black}
\setcounter{tocdepth}{2}
\tableofcontents
}
\subsection{{1. Description}}\label{description}

This project consists of an automated text categorization task
(Sebastiani 2002). The data is classified into 4 genres; ``Child'',
``History'', ``Religion'', and ``Science''. Each category contains
fractions of text files from that genre. ``Child'' category has 7,165
files, ``History'' category has 5,353 files, ``Religion'' category has
2,362 files, and ``Science'' category has 7,432 files. It is plausible
for human to classify the text files from their contexts or specific
words. The goal of this project is to train the machine so that it can
classify the texts as how humans do. We use support vector machine and
random forest methods, test on new text files, and measure the test
error of each method.

\begin{longtable}[c]{@{}ll@{}}
\toprule
Categories & Number.of.files\tabularnewline
\midrule
\endhead
Child & 7,165 files\tabularnewline
History & 5,353 files\tabularnewline
Religion & 2,362 files\tabularnewline
Science & 7,432 files\tabularnewline
\bottomrule
\end{longtable}

\subsection{{2. Feature Creation}}\label{feature-creation}

In order to do text mining, we loaded the text data as a corpus, which
is a collection of separate texts. The tm package provides several text
transformation or word count commands. The tm\_map() command is applied
to the corpus to make transformations such as excluding too common or
rare words, convert every word to lowercase, and remove punctuations or
words. We stemmed the words to consolidate slight variations of word
suffixes into a more concise feature set (Porter 1980). We also removed
the lengthy header and footer text created by Project Gutenberg that
would likely hurt our predictive accuracy.

The DocumentTermMatrix() command is then applied to make a dictionary of
every unique word of the loaded text files and count the appearances of
each word. The outcome is in form of a data frame. Thus each row is the
text file, each column is the unique word, and each cell is the word
count in specific text file. This data frame has a dimension of 22308 *
12262. This word feature matrix approach is commonly known as a bag of
words model (Manning \& Schütze 1999, 7.2).

We then generated a target vector as a factor with values of ``Child'',
``History'', ``Religion'', and ``Science'' which are the tags of the
texts' genre.

\subsection{{3. Unsupervised Feature
Filtering}}\label{unsupervised-feature-filtering}

Before applying the minimum or maximum threshold to the feature matrix,
we check if specific word is included in each text. In other words we
count how many documents use that word. The maximum threshold we applied
is if a word is in at least 70\% of documents. Here we removed 11 words.
The minimum threshold is if a word is in at least 200 documents. The
number of word features we ended with is 7159 features. So our feature
matrix is 22308 * 7159 data frame.

\subsection{{4. Power Feature
Extraction}}\label{power-feature-extraction}

The power features are the variables that help distinguish files in
different categories. These features consist of three kinds: Word-based
power features, sentence-based power features, and N-grams.The following
are the features from each file considered to help prediction.

\begin{enumerate}
\def\labelenumi{(\arabic{enumi})}
\itemsep1pt\parskip0pt\parsep0pt
\item
  Word-based power features: 6 features are collected using the
  unfiltered word matrix, which presents the number of appearance of
  every word in a text.
\end{enumerate}

\begin{longtable}[c]{@{}ll@{}}
\toprule
Power.Feature & Definition\tabularnewline
\midrule
\endhead
words\_count & Number of words\tabularnewline
chars\_count & Number of characters\tabularnewline
words\_avg\_length & Average words length (Number of characters/Number
of words)\tabularnewline
words\_distinct & Number of words that appears in a text at least
once\tabularnewline
sd\_words & Standard deviation of word length\tabularnewline
word\_diversity & Word diversity (Number of distinct words/Number of
words)\tabularnewline
\bottomrule
\end{longtable}

\begin{enumerate}
\def\labelenumi{(\arabic{enumi})}
\setcounter{enumi}{1}
\itemsep1pt\parskip0pt\parsep0pt
\item
  Sentence-based power features: We first splitted the text files into
  sentences and then collected 6 power features by inspecting the list
  of sentences.
\end{enumerate}

\begin{longtable}[c]{@{}ll@{}}
\toprule
Power.Feature & Definition\tabularnewline
\midrule
\endhead
sentence\_count & Number of sentences\tabularnewline
sentence\_avg\_length & Average length of sentences\tabularnewline
4digit\_nums & Number of 4-digit number\tabularnewline
digit\_count & Number of digits\tabularnewline
question\_marks & Number of question marks\tabularnewline
exclamation\_points & Number of exclamation points\tabularnewline
\bottomrule
\end{longtable}

\begin{enumerate}
\def\labelenumi{(\arabic{enumi})}
\setcounter{enumi}{2}
\itemsep1pt\parskip0pt\parsep0pt
\item
  N-grams: The N-gram is a sequence of N words sliced from a longer
  string. The word features we described above are unigrams. It is known
  that the N-gram-based approach to text categorization is tolerant of
  textual errors, fast and robust. This approach works very well for
  language classification (William \& John, 1994). In this project, for
  better prediction, other than unigrams we also included the 4736
  bigram and 497 trigram features. We were able to make a feature matrix
  of N-grams with the DocumentTermMatrix() command as we did for the
  word features but by adjusting the command with the N-gram-tokenizer.
\end{enumerate}

As a result, we have 5245 power features in total.

\subsection{{5. Word and Power Feature
Combination}}\label{word-and-power-feature-combination}

22308 * 7159 22305 * 5245

\subsection{{6. Classification of Word Feature
Matrix}}\label{classification-of-word-feature-matrix}

\subsection{{7. Classification of Power Word Feature
Matrix}}\label{classification-of-power-word-feature-matrix}

\subsection{{8. Classification of Combined Feature
Matrix}}\label{classification-of-combined-feature-matrix}

\subsection{{9. Final Model}}\label{final-model}

\subsection{{10. Validation Set}}\label{validation-set}

\subsection{{11. References}}\label{references}

Manning, C. D., \& Schütze, H. (1999). Foundations of statistical
natural language processing. MIT press.

Porter, M. F. (1980). An algorithm for suffix stripping. Program, 14(3),
130-137.

Sebastiani, F. (2002). Machine learning in automated text
categorization. ACM computing surveys (CSUR), 34(1), 1-47.

William, B. Cavnar \& John M. Trenkle. (1994). N-Gram-Based Text
Categorization. Environmental Research Institute of Michigan.

\end{document}
