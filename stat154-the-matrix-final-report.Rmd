---
title: STAT 154 Final Report
author: |
    | The Matrix (Team 5)
    | Stephanie Kim, Chris Kennedy, Dongkyun Kim, Saejin Kim, Zhenzheng Hu
date: "12/6/2015"
output:
  pdf_document:
    fig_caption: yes
    keep_tex: yes
---

```{r, echo=F, message=F, warning=F}
library(knitr)
```

## <span style="color:black">1. Description</span>
This project consists of an automated text categorization task (Sebastiani 2002). The data is classified into 4 genres; “Child”, “History”, “Religion”, and “Science”. Each category contains fractions of text files from that genre. “Child” category has 7,165 files, “History” category has 5,353 files, “Religion” category has 2,362 files, and “Science” category has 7,432 files. It is plausible for human to classify the text files from their contexts or specific words. The goal of this project is to train the machine so that it can classify the texts as how humans do. We use support vector machine and random forest methods, test on new text files, and measure the test error of each method.

```{r, echo=F, message=F, warning=F}
df = data.frame("Categories"=c("Child","History","Religion","Science"),
                "Number of files"=c("7,165 files","5,353 files","2,362 files","7,432 files"))
kable(df)
```

## <span style="color:black">2. Feature Creation</span>

In order to do text mining, we loaded the text data as a corpus, which is a collection of separate texts. The tm package provides several text transformation or word count commands. The tm_map() command is applied to the corpus to make transformations such as excluding too common or rare words, convert every word to lowercase, and remove punctuations or words. We stemmed the words to consolidate slight variations of word suffixes into a more concise feature set (Porter 1980). We also removed the lengthy header and footer text created by Project Gutenberg that would likely hurt our predictive accuracy.

The DocumentTermMatrix() command is then applied to make a dictionary of every unique word of the loaded text files and count the appearances of each word.  The outcome is in form of a data frame. Thus each row is the text file, each column is the unique word, and each cell is the word count in specific text file (22308 * 12262 data frame). This word feature matrix approach is commonly known as a bag of words model (Manning &  Schütze 1999, 7.2).

We then generated a target vector as a factor with values of “Child”, “History”, “Religion”, and “Science” which are the tags of the texts’ genre.

```{r, echo=FALSE}

```
## <span style="color:black">3. Unsupervised Feature Filtering</span>

Before applying the minimum or maximum threshold to the feature matrix, we check if specific word is included in each text. In other words we count how many documents use that word. The maximum threshold we applied is if a word is in at least 70% of documents. Here we removed 11 words. The minimum threshold is if a word is in at least 200 documents. The number of word features we ended with is 7159 features. So our feature matrix is 22308 * 7159 data frame.

```{r, echo=F, message=F, warning=F}
```

## <span style="color:black">4. Power Feature Extraction</span>

The power features are the variables that help distinguish files in different categories. These features consist of three kinds: Word-based power features, sentence-based power features, and N-grams.The following are the features from each file considered to help prediction. 

Before applying the minimum or maximum threshold to the feature matrix, we check if specific word is included in each text. In other words we count how many documents use that word. The maximum threshold we applied is if a word is in at least 70% of documents. Here we removed 11 words. The minimum threshold is if a word is in at least 200 documents. The number of word features we ended with is 7159 features. So our feature matrix is 22308 * 7159 data frame.
```{r, echo=FALSE}

```

## <span style="color:black">5. Classification of Power Word Feature Matrix</span>
(1) Word-based power features: Features are collected using the unfiltered word matrix, which presents the number of appearance of every word in a text.
 
```{r, echo=F, message=F, warning=F}
```
 
(2) Sentence-based power features: We first splitted the text files into sentences and then collected the power features by inspecting the list of sentences.

```{r, echo=F, message=F, warning=F}
```

(3) N-grams: N-gram is a sequence of N words sliced from a longer string. The word features we described above are unigrams. It is known that the N-gram-based approach to text categorization is tolerant of textual errors, fast and robust. This approach works very well for language classification (William & John, 1994). In this project, for better prediction, other than unigrams we also included the bigram trigram features. We were able to make a feature matrix of N-grams with the DocumentTermMatrix() command as we did for the word features but by adjusting the command with the N-gram-tokenizer.

## <span style="color:black">5. Word and Power Feature Combination</span>

## <span style="color:black">6. Classification of Word Feature Matrix</span>

```{r, echo=F, message=F, warning=F}

```

## <span style="color:black">7. Classification of Power Word Feature Matrix</span>

```{r, echo=F, message=F, warning=F}

```

## <span style="color:black">8. Classification of Combined Feature Matrix</span>

```{r, echo=F, message=F, warning=F}

```

## <span style="color:black">9. Final Model</span>

```{r, echo=F, message=F, warning=F}

```

## <span style="color:black">10. Validation Set</span>

```{r, echo=F, message=F, warning=F}

```

## <span style="color:black">11. References</span>
```{r, echo=FALSE}

```