---
title: STAT 154 Final Report
author: |
    | The Matrix (Team5)
    | Stephanie Kim (25937115), Chris Kennedy (25947847), 
    | Dongkyun Kim (22874163), Saejin Kim (00000000), Zhenzheng Hu (23715262)
date: "12/7/2015"
output:
  pdf_document:
    fig_caption: yes
    keep_tex: yes
    toc: true
    toc_depth: 2
fontsize: 12pt
---

```{r, echo=F, message=F, warning=F}
library(knitr)
```

## <span style="color:black">1. Description</span>

This project consists of an automated text categorization task (Sebastiani 2002) applied to text documents collected from Project Gutenberg, a free ebook archive. The documents are categorized into 4 genres: “Child”, “History”, “Religion”, and “Science”. A human would be able to classify these documents without much effort but it would be very time confusing to do so for thousands or millions of documents. The goal of this project is to develop a machine learning classifier to correctly categorize the texts as accurately as possible. Our primary feature set is a word-feature matrix which counts the number of a times each word occurs in a document, known as a bag of words model. We extend this word feature matrix with bigrams, trigrams, and meta-features related to statistics on word and character usage. We use support vector machine and random forest methods, test on new text files, and measure the test error of each method. Our final model is a gradient boosted machine.

```{r, echo=F, message=F, warning=F}
df = data.frame(c("Child", "History", "Religion", "Science"),
                c("7,165", "5,353", "2,362", "7,432"))
names(df) = c("Categories", "Number of files")
kable(df, caption="Documents in each category")
```

## <span style="color:black">2. Feature Creation</span>

In order to do text mining, we loaded the text data as a corpus, which is a collection of separate texts. The tm package provides several text transformation or word count commands. The tm_map() command is applied to the corpus to make transformations such as excluding too common or rare words, convert every word to lowercase, and remove punctuations or words. We stemmed the words to consolidate slight variations of word suffixes into a more concise feature set (Porter 1980). Terms that occurred in fewer than 1% of documents were excluded at this stage.

We also removed the lengthy header and footer text created by Project Gutenberg that would likely hurt our predictive accuracy. This was particularly time-consuming because there is no standard string that separates the meta-data from the literary data in Project Gutenberg documents; we strongly recommend that Project Gutenberg develop such a standard to facilitate comptuer processing of its documents in the future. Moreover, a set of documents, roughly 600 or more, consisted entirely of Project Gutenberg legal or other non-literary text. We removed some of those documents as training examples although we would have liked to spend more time removing the remainder (e.g. using outlier detection methods) so that their feature distributions would not influence the prediction algorithms.

The DocumentTermMatrix() command is then applied to make a dictionary of every unique word of the loaded text files and count the appearances of each word.  The outcome is in the form of a data frame, where each row is a text document, each column is a unique word, and each cell is the count of that word in the text document. The resulting data frame has 22,305 rows and  12,196 columns. This word feature matrix approach is commonly known as a bag of words representation (Manning &  Schütze 1999, 7.2).

We then generated a target vector as a factor with values of “Child”, “History”, “Religion”, and “Science” which are the tags of the texts’ genre.

```{r, echo=FALSE}

```
## <span style="color:black">3. Unsupervised Feature Filtering</span>

In order to reduce the feature set size and limit overfitting, we tally how many documents use each word. We chose a minimum threshold of 50 documents; any word with less usage was removed from the feature matrix. We selected a relatively low minimum so that the machine learning algorithms could achieve maximum accuracy through a large feature set - aggressive pruning would limit our ability to detect patterns in word usage. Similarly, we chose not to limit on the maximum number of documents using a feature under the theory that words with high document coverage could still provide useful differentiation across categories. Our final word feature matrix contains 22,305 rows and 11,218 word-feature columns.

[Plot of word frequency]

```{r, echo=F, message=F, warning=F}
```

## <span style="color:black">4. Power Feature Extraction</span>

The power features are the variables that help distinguish files in different categories. These features consist of three kinds: Word-based power features, sentence-based power features, and N-grams.The following are the features from each file considered to help prediction. 

(1) Word-based power features: 6 features are collected using the unfiltered word matrix, which presents the number of appearance of every word in a text.
 
```{r, echo=F, message=F, warning=F}
df2 = data.frame(c("words_count","chars_count","words_avg_length","words_distinct","sd_words","word_diversity"),
                c("Number of words","Number of characters","Average words length (Number of characters/Number of words)","Number of words that appears in a text at least once","Standard deviation of word length","Word diversity (Number of distinct words/Number of words)"))
names(df2) = c("Power Feature", "Definition")
kable(df2,caption="Word-based power features")
```
 
(2) Sentence-based power features: We first split the text files into sentences and then collected 6 power features by inspecting the list of sentences.

```{r, echo=F, message=F, warning=F}
df3 = data.frame(c("sentence_count","sentence_avg_length","4digit_nums","digit_count","question_marks","exclamation_points"),
                c("Number of sentences","Average length of sentences","Number of 4-digit number","Number of digits","Number of question marks","Number of exclamation points"))
names(df3) = c("Power Feature", "Definition")
kable(df3,caption="Sentence-based power features")
```

(3) N-grams: The N-gram is a sequence of N words sliced from a longer string. The word features we described above are unigrams. It is known that the N-gram-based approach to text categorization is tolerant of textual errors, fast and robust. This approach works very well for language classification (William & John, 1994). In this project, for better prediction, other than unigrams we also included the 4,736 bigram and 497 trigram features. We were able to make a feature matrix of N-grams with the DocumentTermMatrix() command as we did for the word features but by adjusting the command with the N-gram-tokenizer. The trigram features were especially useful for identifying documents that only included Project Gutenberg meta text, because non-literary legal phrases tended to occur frequently in those documents.

As a result, we have 5,245 power features in total. The power feature matrix has dimension 22,305 $\times$ 5,245.

## <span style="color:black">5. Word and Power Feature Combination</span>

We combine our word and power features to generate a final feature matrix of 22,305 documents and 16,463 features.

## <span style="color:black">6. Classification of Word Feature Matrix</span>

```{r, echo=F, message=F, warning=F}

```

## <span style="color:black">7. Classification of Power Word Feature Matrix</span>

```{r, echo=F, message=F, warning=F}

```

## <span style="color:black">8. Classification of Combined Feature Matrix</span>

```{r, echo=F, message=F, warning=F}

```

## <span style="color:black">9. Final Model</span>

```{r, echo=F, message=F, warning=F}

```

## <span style="color:black">10. Validation Set</span>

```{r, echo=F, message=F, warning=F}

```

## <span style="color:black">11. References</span>
Manning, C. D., & Schütze, H. (1999). Foundations of statistical natural language processing. MIT press.

Porter, M. F. (1980). An algorithm for suffix stripping. Program, 14(3), 130-137.

Sebastiani, F. (2002). Machine learning in automated text categorization. ACM computing surveys (CSUR), 34(1), 1-47.

William, B. Cavnar & John M. Trenkle. (1994). N-Gram-Based Text Categorization. Environmental Research Institute of Michigan.